{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2q27gKz1H20"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "!pwd\n",
        "!cp -r /content/drive/MyDrive/labeled_chess ."
      ],
      "metadata": {
        "id": "-SWOQkcdb5d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "839fc3f7-acc8-439c-a829-104fd6b4f9b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUfAcER1oUS6"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7qyhNL1yWt"
      },
      "source": [
        "# Object Detection with TensorFlow Lite Model Maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw5Y7snSuG51"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/models/modify/model_maker/object_detection\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/tensorflow/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr3q-gvm3cI8"
      },
      "source": [
        "In this colab notebook, you'll learn how to use the [TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/models/modify/model_maker) library to train a custom object detection model capable of detecting salads within images on a mobile device.\n",
        "\n",
        "The Model Maker library uses *transfer learning* to simplify the process of training a TensorFlow Lite model using a custom dataset. Retraining a TensorFlow Lite model with your own custom dataset reduces the amount of training data required and will shorten the training time.\n",
        "\n",
        "You'll use the publicly available *Salads* dataset, which was created from the [Open Images Dataset V4](https://storage.googleapis.com/openimages/web/index.html).\n",
        "\n",
        "Each image in the dataset \bcontains objects labeled as one of the following classes:\n",
        "* Baked Good\n",
        "* Cheese\n",
        "* Salad\n",
        "* Seafood\n",
        "* Tomato\n",
        "\n",
        "The dataset contains the bounding-boxes specifying where each object locates, together with the object's label.\n",
        "\n",
        "Here is an example image from the dataset:\n",
        "\n",
        "<br/>\n",
        "\n",
        "<img src=\"https://cloud.google.com/vision/automl/object-detection/docs/images/quickstart-preparing_a_dataset.png\" width=\"400\" hspace=\"0\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLF2PKkSbV3"
      },
      "source": [
        "## Prerequisites\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vvAObmTqglq"
      },
      "source": [
        "### Install the required packages\n",
        "Start by installing the required packages, including the Model Maker package from the [GitHub repo](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker) and the pycocotools library you'll use for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhl8lqVamEty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdda5d4f-673b-4013-f53a-500accf11aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 64.6 kB of archives.\n",
            "After this operation, 215 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libportaudio2 amd64 19.6.0-1 [64.6 kB]\n",
            "Fetched 64.6 kB in 1s (106 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 155639 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "\u001b[K     |████████████████████████████████| 642 kB 8.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 58.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 65.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 51.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 22.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 30.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 52.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 237 kB 75.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 55.1 MB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 840 kB 64.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 87 kB 570 kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 55.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 213 kB 71.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 66.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 73.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 48.3 MB 104 kB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 10.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 1.3 MB/s \n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's legacy dependency resolver does not consider dependency conflicts when selecting packages. This behaviour is the source of the following dependency conflicts.\n",
            "tflite-support-nightly 0.4.0.dev20220704 requires flatbuffers>=2.0, but you'll have flatbuffers 1.12 which is incompatible.\n",
            "tflite-support-nightly 0.4.0.dev20220704 requires protobuf<4,>=3.18.0, but you'll have protobuf 3.17.3 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 1.4 MB/s \n",
            "\u001b[?25hFound existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n",
            "Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n",
            "  Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n",
            "\u001b[K     |████████████████████████████████| 668.3 MB 17 kB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 6.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!sudo apt -y install libportaudio2\n",
        "!pip install -q --use-deprecated=legacy-resolver tflite-model-maker-nightly\n",
        "!pip install -q pycocotools\n",
        "!pip install -q opencv-python-headless==4.1.2.30\n",
        "!pip uninstall -y tensorflow && pip install -q tensorflow==2.8.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tflite-model-maker-nightly"
      ],
      "metadata": {
        "id": "UEZHAUyV0kci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60f300ef-2e12-488c-9fcc-8598731583d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tflite-model-maker-nightly in /usr/local/lib/python3.7/dist-packages (0.4.2.dev202207050507)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (0.1.96)\n",
            "Requirement already satisfied: tensorflow-model-optimization>=0.5 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (0.7.2)\n",
            "Requirement already satisfied: Cython>=0.29.13 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (0.29.30)\n",
            "Requirement already satisfied: scann==1.2.6 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (1.2.6)\n",
            "Requirement already satisfied: tensorflow-datasets>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (4.0.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (1.21.6)\n",
            "Requirement already satisfied: absl-py>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (1.1.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (6.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers==1.12 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (1.12)\n",
            "Requirement already satisfied: tensorflow>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (2.8.0+zzzcolab20220506162203)\n",
            "Requirement already satisfied: tf-models-official==2.3.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (2.3.0)\n",
            "Requirement already satisfied: librosa==0.8.1 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (0.8.1)\n",
            "Requirement already satisfied: fire>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-hub<0.13,>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (0.12.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (1.24.3)\n",
            "Requirement already satisfied: tfa-nightly in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (0.17.0.dev20220703191645)\n",
            "Requirement already satisfied: neural-structured-learning>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (1.3.1)\n",
            "Requirement already satisfied: matplotlib<3.5.0,>=3.0.3 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (3.2.2)\n",
            "Requirement already satisfied: pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (7.1.2)\n",
            "Requirement already satisfied: lxml>=4.6.1 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (4.9.1)\n",
            "Requirement already satisfied: numba==0.53 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (0.53.0)\n",
            "Requirement already satisfied: tflite-support-nightly in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker-nightly) (0.4.0.dev20220704)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker-nightly) (1.1.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker-nightly) (2.1.9)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker-nightly) (0.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker-nightly) (21.3)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker-nightly) (1.6.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker-nightly) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker-nightly) (1.0.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker-nightly) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker-nightly) (0.10.3.post1)\n",
            "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /usr/local/lib/python3.7/dist-packages (from numba==0.53->tflite-model-maker-nightly) (0.36.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.53->tflite-model-maker-nightly) (57.4.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker-nightly) (4.1.2.30)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker-nightly) (8.0.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker-nightly) (1.5.12)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker-nightly) (5.4.8)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker-nightly) (1.3.5)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker-nightly) (0.5.0)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker-nightly) (1.21.0)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker-nightly) (1.1.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker-nightly) (1.12.11)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker-nightly) (0.6)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker-nightly) (0.17.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire>=0.3.1->tflite-model-maker-nightly) (1.1.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker-nightly) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker-nightly) (3.0.1)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker-nightly) (1.35.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker-nightly) (0.17.4)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker-nightly) (1.31.6)\n",
            "Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker-nightly) (3.17.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker-nightly) (2.23.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker-nightly) (2022.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker-nightly) (1.56.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker-nightly) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker-nightly) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker-nightly) (0.2.8)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.3.0->tflite-model-maker-nightly) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.3.0->tflite-model-maker-nightly) (0.4.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0->tflite-model-maker-nightly) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0->tflite-model-maker-nightly) (2022.6.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0->tflite-model-maker-nightly) (4.64.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0->tflite-model-maker-nightly) (6.1.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.5.0,>=3.0.3->tflite-model-maker-nightly) (1.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.5.0,>=3.0.3->tflite-model-maker-nightly) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.5.0,>=3.0.3->tflite-model-maker-nightly) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<3.5.0,>=3.0.3->tflite-model-maker-nightly) (4.1.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from neural-structured-learning>=1.3.1->tflite-model-maker-nightly) (21.4.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.1->tflite-model-maker-nightly) (1.4.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker-nightly) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker-nightly) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker-nightly) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.8.1->tflite-model-maker-nightly) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa==0.8.1->tflite-model-maker-nightly) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.8.1->tflite-model-maker-nightly) (2.21)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker-nightly) (1.46.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker-nightly) (2.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker-nightly) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker-nightly) (1.14.1)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker-nightly) (14.0.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker-nightly) (1.6.3)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker-nightly) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker-nightly) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker-nightly) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker-nightly) (0.26.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker-nightly) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker-nightly) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker-nightly) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.6.0->tflite-model-maker-nightly) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.6.0->tflite-model-maker-nightly) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker-nightly) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker-nightly) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker-nightly) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker-nightly) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker-nightly) (0.6.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker-nightly) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker-nightly) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker-nightly) (3.8.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker-nightly) (3.2.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker-nightly) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker-nightly) (1.8.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker-nightly) (0.1.7)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker-nightly) (0.3.5.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker-nightly) (0.16.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker-nightly) (5.7.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.3.0->tflite-model-maker-nightly) (1.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official==2.3.0->tflite-model-maker-nightly) (2.7.1)\n",
            "Collecting protobuf<4.0.0dev,>=3.12.0\n",
            "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 6.7 MB/s \n",
            "\u001b[?25hCollecting tflite-support-nightly\n",
            "  Downloading tflite_support_nightly-0.4.0.dev20220703-cp37-cp37m-manylinux2014_x86_64.whl (55.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 55.1 MB 1.6 MB/s \n",
            "\u001b[?25h  Downloading tflite_support_nightly-0.4.0.dev20220702-cp37-cp37m-manylinux2014_x86_64.whl (55.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 55.1 MB 112 kB/s \n",
            "\u001b[?25h  Downloading tflite_support_nightly-0.4.0.dev20220701-cp37-cp37m-manylinux2014_x86_64.whl (55.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 55.1 MB 46 kB/s \n",
            "\u001b[?25h  Downloading tflite_support_nightly-0.4.0.dev20220630-cp37-cp37m-manylinux2014_x86_64.whl (42.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.6 MB 1.4 MB/s \n",
            "\u001b[?25h  Downloading tflite_support_nightly-0.4.0.dev20220629-cp37-cp37m-manylinux2014_x86_64.whl (42.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.6 MB 1.2 MB/s \n",
            "\u001b[?25h  Downloading tflite_support_nightly-0.4.0.dev20220628-cp37-cp37m-manylinux2014_x86_64.whl (42.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.6 MB 1.1 MB/s \n",
            "\u001b[?25h  Downloading tflite_support_nightly-0.4.0.dev20220627-cp37-cp37m-manylinux2014_x86_64.whl (42.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.6 MB 1.1 MB/s \n",
            "\u001b[?25h  Downloading tflite_support_nightly-0.4.0.dev20220626-cp37-cp37m-manylinux2014_x86_64.whl (42.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.6 MB 1.2 MB/s \n",
            "\u001b[?25h  Downloading tflite_support_nightly-0.4.0.dev20220625-cp37-cp37m-manylinux2014_x86_64.whl (42.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.6 MB 1.4 MB/s \n",
            "\u001b[?25h  Downloading tflite_support_nightly-0.4.0.dev20220624-cp37-cp37m-manylinux2014_x86_64.whl (42.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.6 MB 1.2 MB/s \n",
            "\u001b[?25h  Downloading tflite_support_nightly-0.4.0.dev20220623-cp37-cp37m-manylinux2014_x86_64.whl (42.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.6 MB 1.4 MB/s \n",
            "\u001b[?25h  Downloading tflite_support_nightly-0.4.0.dev20220622-cp37-cp37m-manylinux2014_x86_64.whl (42.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.6 MB 1.2 MB/s \n",
            "\u001b[?25h  Downloading tflite_support_nightly-0.4.0.dev20220621-cp37-cp37m-manylinux2014_x86_64.whl (42.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.6 MB 1.3 MB/s \n",
            "\u001b[?25h  Downloading tflite_support_nightly-0.4.0.dev20220620-cp37-cp37m-manylinux2014_x86_64.whl (42.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.6 MB 1.3 MB/s \n",
            "\u001b[?25h  Downloading tflite_support_nightly-0.4.0.dev20220619-cp37-cp37m-manylinux2014_x86_64.whl (42.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.6 MB 178 kB/s \n",
            "\u001b[?25h  Downloading tflite_support_nightly-0.4.0.dev20220618-cp37-cp37m-manylinux2014_x86_64.whl (42.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.6 MB 1.3 MB/s \n",
            "\u001b[?25h  Downloading tflite_support_nightly-0.4.0.dev20220617-cp37-cp37m-manylinux2014_x86_64.whl (42.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.6 MB 135 kB/s \n",
            "\u001b[?25h  Downloading tflite_support_nightly-0.4.0.dev20220616-cp37-cp37m-manylinux2014_x86_64.whl (42.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.6 MB 1.3 MB/s \n",
            "\u001b[?25h  Downloading tflite_support_nightly-0.4.0.dev20220608-cp37-cp37m-manylinux2014_x86_64.whl (42.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.5 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from tflite-support-nightly->tflite-model-maker-nightly) (2.9.2)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.7/dist-packages (from tflite-support-nightly->tflite-model-maker-nightly) (0.4.4)\n",
            "Installing collected packages: protobuf, tflite-support-nightly\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: tflite-support-nightly\n",
            "    Found existing installation: tflite-support-nightly 0.4.0.dev20220704\n",
            "    Uninstalling tflite-support-nightly-0.4.0.dev20220704:\n",
            "      Successfully uninstalled tflite-support-nightly-0.4.0.dev20220704\n",
            "Successfully installed protobuf-3.20.1 tflite-support-nightly-0.4.0.dev20220608\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6lRhVK9Q_0U"
      },
      "source": [
        "Import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtxiUeZEiXpt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tflite_model_maker.config import QuantizationConfig\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRd13bfetO7B"
      },
      "source": [
        "### Prepare the dataset\n",
        "\n",
        "Here you'll use the same dataset as the AutoML [quickstart](https://cloud.google.com/vision/automl/object-detection/docs/edge-quickstart#preparing_a_dataset).\n",
        "\n",
        "The *Salads* dataset is available at:\n",
        " `gs://cloud-ml-data/img/openimage/csv/salads_ml_use.csv`.\n",
        "\n",
        "It contains 175 images for training, 25 images for validation, and 25 images for testing. The dataset has five classes: `Salad`, `Seafood`, `Tomato`, `Baked goods`, `Cheese`.\n",
        "\n",
        "<br/>\n",
        "\n",
        "The dataset is provided in CSV format:\n",
        "```\n",
        "TRAINING,gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg,Salad,0.0,0.0954,,,0.977,0.957,,\n",
        "VALIDATION,gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg,Seafood,0.0154,0.1538,,,1.0,0.802,,\n",
        "TEST,gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg,Tomato,0.0,0.655,,,0.231,0.839,,\n",
        "```\n",
        "\n",
        "* Each row corresponds to an object localized inside a larger image, with each object specifically designated as test, train, or validation data. You'll learn more about what that means in a later stage in this notebook.\n",
        "* The three lines included here indicate **three distinct objects located inside the same image** available at `gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg`.\n",
        "* Each row has a different label: `Salad`, `Seafood`, `Tomato`, etc.\n",
        "* Bounding boxes are specified for each image using the top left and bottom right vertices.\n",
        "\n",
        "Here is a visualzation of these three lines:\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://cloud.google.com/vision/automl/object-detection/docs/images/quickstart-preparing_a_dataset.png\" width=\"400\" hspace=\"100\">\n",
        "\n",
        "If you want to know more about how to prepare your own CSV file and the minimum requirements for creating a valid dataset, see the [Preparing your training data](https://cloud.google.com/vision/automl/object-detection/docs/prepare) guide for more details.\n",
        "\n",
        "If you are new to Google Cloud, you may wonder what the `gs://` URL means. They are URLs of files stored on [Google Cloud Storage](https://cloud.google.com/storage) (GCS). If you make your files on GCS public or [authenticate your client](https://cloud.google.com/storage/docs/authentication#libauth), Model Maker can read those files similarly to your local files.\n",
        "\n",
        "However, you don't need to keep your images on Google Cloud to use Model Maker. You can use a local path in your CSV file and Model Maker will just work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xushUyZXqP59"
      },
      "source": [
        "## Quickstart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn61LJ9QbOPi"
      },
      "source": [
        "There are six steps to training an object detection model:\n",
        "\n",
        "**Step 1. Choose an object detection model archiecture.**\n",
        "\n",
        "This tutorial uses the EfficientDet-Lite0 model. EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture.\n",
        "\n",
        "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
        "\n",
        "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
        "|--------------------|-----------|---------------|----------------------|\n",
        "| EfficientDet-Lite0 | 4.4       | 37            | 25.69%               |\n",
        "| EfficientDet-Lite1 | 5.8       | 49            | 30.55%               |\n",
        "| EfficientDet-Lite2 | 7.2       | 69            | 33.97%               |\n",
        "| EfficientDet-Lite3 | 11.4      | 116           | 37.70%               |\n",
        "| EfficientDet-Lite4 | 19.9      | 260           | 41.96%               |\n",
        "\n",
        "<i> * Size of the integer quantized models. <br/>\n",
        "** Latency measured on Pixel 4 using 4 threads on CPU. <br/>\n",
        "*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n",
        "</i>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtdZ-JDwMimd"
      },
      "outputs": [],
      "source": [
        "spec = model_spec.get('efficientdet_lite0')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "kb6TNCHfWT-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5U-A3tw6Y27"
      },
      "source": [
        "**Step 2. Load the dataset.**\n",
        "\n",
        "Model Maker will take input data in the CSV format. Use the `object_detector.DataLoader.from_csv` method to load the dataset and split them into the training, validation and test images.\n",
        "\n",
        "* Training images: These images are used to train the object detection model to recognize salad ingredients.\n",
        "* Validation images: These are images that the model didn't see during the training process. You'll use them to decide when you should stop the training, to avoid [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n",
        "* Test images: These images are used to evaluate the final model performance.\n",
        "\n",
        "You can load the CSV file directly from Google Cloud Storage, but you don't need to keep your images on Google Cloud to use Model Maker. You can specify a local CSV file on your computer, and Model Maker will work just fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD5BvzWe6YKa"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"/content/labeled_chess/train/\")\n",
        "train_data, _, _ = object_detector.DataLoader.from_csv('_annotations.csv')\n",
        "os.chdir(\"/content/labeled_chess/valid/\")\n",
        "_, validation_data, _ = object_detector.DataLoader.from_csv('_annotations.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gsutil cp gs://cloud-ml-data/img/openimage/csv/salads_ml_use.csv ./\n",
        "print(len(train_data),len(validation_data))\n"
      ],
      "metadata": {
        "id": "xH8mofgBSTO0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63f26832-411d-42ee-a227-5e58f63a97f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "606 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uZkLR6N6gDR"
      },
      "source": [
        "**Step 3. Train the TensorFlow model with the training data.**\n",
        "\n",
        "* The EfficientDet-Lite0 model uses `epochs = 50` by default, which means it will go through the training dataset 50 times. You can look at the validation accuracy during training and stop early to avoid overfitting.\n",
        "* Set `batch_size = 8` here so you will see that it takes 21 steps to go through the 175 images in the training dataset.\n",
        "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwlYdTcg63xy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fde4af60-b9fe-4fa0-d8be-be153839ea03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "75/75 [==============================] - 60s 271ms/step - det_loss: 1.4748 - cls_loss: 0.9751 - box_loss: 0.0100 - reg_l2_loss: 0.0650 - loss: 1.5398 - learning_rate: 0.0090 - gradient_norm: 1.4980 - val_det_loss: 1.1174 - val_cls_loss: 0.8726 - val_box_loss: 0.0049 - val_reg_l2_loss: 0.0650 - val_loss: 1.1824\n",
            "Epoch 2/40\n",
            "75/75 [==============================] - 19s 252ms/step - det_loss: 1.0090 - cls_loss: 0.7387 - box_loss: 0.0054 - reg_l2_loss: 0.0650 - loss: 1.0740 - learning_rate: 0.0100 - gradient_norm: 2.0285 - val_det_loss: 0.8879 - val_cls_loss: 0.7144 - val_box_loss: 0.0035 - val_reg_l2_loss: 0.0650 - val_loss: 0.9529\n",
            "Epoch 3/40\n",
            "75/75 [==============================] - 17s 231ms/step - det_loss: 0.8105 - cls_loss: 0.5932 - box_loss: 0.0043 - reg_l2_loss: 0.0650 - loss: 0.8756 - learning_rate: 0.0099 - gradient_norm: 2.0324 - val_det_loss: 0.9129 - val_cls_loss: 0.7813 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.0651 - val_loss: 0.9780\n",
            "Epoch 4/40\n",
            "75/75 [==============================] - 18s 242ms/step - det_loss: 0.6890 - cls_loss: 0.5176 - box_loss: 0.0034 - reg_l2_loss: 0.0651 - loss: 0.7540 - learning_rate: 0.0098 - gradient_norm: 1.7487 - val_det_loss: 0.9619 - val_cls_loss: 0.8380 - val_box_loss: 0.0025 - val_reg_l2_loss: 0.0651 - val_loss: 1.0270\n",
            "Epoch 5/40\n",
            "75/75 [==============================] - 26s 347ms/step - det_loss: 0.6504 - cls_loss: 0.4820 - box_loss: 0.0034 - reg_l2_loss: 0.0651 - loss: 0.7155 - learning_rate: 0.0097 - gradient_norm: 1.9000 - val_det_loss: 0.9779 - val_cls_loss: 0.8567 - val_box_loss: 0.0024 - val_reg_l2_loss: 0.0651 - val_loss: 1.0430\n",
            "Epoch 6/40\n",
            "75/75 [==============================] - 18s 236ms/step - det_loss: 0.5840 - cls_loss: 0.4213 - box_loss: 0.0033 - reg_l2_loss: 0.0651 - loss: 0.6491 - learning_rate: 0.0095 - gradient_norm: 1.8955 - val_det_loss: 1.2494 - val_cls_loss: 1.0454 - val_box_loss: 0.0041 - val_reg_l2_loss: 0.0651 - val_loss: 1.3145\n",
            "Epoch 7/40\n",
            "75/75 [==============================] - 18s 245ms/step - det_loss: 0.5110 - cls_loss: 0.3706 - box_loss: 0.0028 - reg_l2_loss: 0.0651 - loss: 0.5761 - learning_rate: 0.0093 - gradient_norm: 1.8290 - val_det_loss: 1.1021 - val_cls_loss: 1.0194 - val_box_loss: 0.0017 - val_reg_l2_loss: 0.0651 - val_loss: 1.1672\n",
            "Epoch 8/40\n",
            "75/75 [==============================] - 18s 243ms/step - det_loss: 0.4857 - cls_loss: 0.3564 - box_loss: 0.0026 - reg_l2_loss: 0.0651 - loss: 0.5509 - learning_rate: 0.0091 - gradient_norm: 1.7333 - val_det_loss: 1.2608 - val_cls_loss: 1.1531 - val_box_loss: 0.0022 - val_reg_l2_loss: 0.0651 - val_loss: 1.3259\n",
            "Epoch 9/40\n",
            "75/75 [==============================] - 18s 239ms/step - det_loss: 0.4783 - cls_loss: 0.3463 - box_loss: 0.0026 - reg_l2_loss: 0.0651 - loss: 0.5435 - learning_rate: 0.0089 - gradient_norm: 1.9079 - val_det_loss: 1.3206 - val_cls_loss: 1.1716 - val_box_loss: 0.0030 - val_reg_l2_loss: 0.0651 - val_loss: 1.3857\n",
            "Epoch 10/40\n",
            "75/75 [==============================] - 20s 266ms/step - det_loss: 0.4679 - cls_loss: 0.3290 - box_loss: 0.0028 - reg_l2_loss: 0.0652 - loss: 0.5330 - learning_rate: 0.0086 - gradient_norm: 1.9194 - val_det_loss: 1.3317 - val_cls_loss: 1.2570 - val_box_loss: 0.0015 - val_reg_l2_loss: 0.0652 - val_loss: 1.3968\n",
            "Epoch 11/40\n",
            "75/75 [==============================] - 18s 245ms/step - det_loss: 0.4443 - cls_loss: 0.3097 - box_loss: 0.0027 - reg_l2_loss: 0.0652 - loss: 0.5095 - learning_rate: 0.0083 - gradient_norm: 1.9581 - val_det_loss: 1.3658 - val_cls_loss: 1.2872 - val_box_loss: 0.0016 - val_reg_l2_loss: 0.0652 - val_loss: 1.4310\n",
            "Epoch 12/40\n",
            "75/75 [==============================] - 18s 239ms/step - det_loss: 0.4014 - cls_loss: 0.2864 - box_loss: 0.0023 - reg_l2_loss: 0.0652 - loss: 0.4666 - learning_rate: 0.0080 - gradient_norm: 1.5783 - val_det_loss: 1.4401 - val_cls_loss: 1.3772 - val_box_loss: 0.0013 - val_reg_l2_loss: 0.0652 - val_loss: 1.5053\n",
            "Epoch 13/40\n",
            "75/75 [==============================] - 18s 238ms/step - det_loss: 0.3907 - cls_loss: 0.2793 - box_loss: 0.0022 - reg_l2_loss: 0.0652 - loss: 0.4559 - learning_rate: 0.0077 - gradient_norm: 1.6468 - val_det_loss: 1.4368 - val_cls_loss: 1.3170 - val_box_loss: 0.0024 - val_reg_l2_loss: 0.0652 - val_loss: 1.5020\n",
            "Epoch 14/40\n",
            "75/75 [==============================] - 18s 235ms/step - det_loss: 0.3848 - cls_loss: 0.2718 - box_loss: 0.0023 - reg_l2_loss: 0.0652 - loss: 0.4500 - learning_rate: 0.0073 - gradient_norm: 1.8775 - val_det_loss: 1.3701 - val_cls_loss: 1.3058 - val_box_loss: 0.0013 - val_reg_l2_loss: 0.0652 - val_loss: 1.4353\n",
            "Epoch 15/40\n",
            "75/75 [==============================] - 21s 278ms/step - det_loss: 0.3688 - cls_loss: 0.2622 - box_loss: 0.0021 - reg_l2_loss: 0.0652 - loss: 0.4340 - learning_rate: 0.0070 - gradient_norm: 1.5422 - val_det_loss: 1.4469 - val_cls_loss: 1.3836 - val_box_loss: 0.0013 - val_reg_l2_loss: 0.0652 - val_loss: 1.5121\n",
            "Epoch 16/40\n",
            "75/75 [==============================] - 18s 235ms/step - det_loss: 0.3881 - cls_loss: 0.2654 - box_loss: 0.0025 - reg_l2_loss: 0.0652 - loss: 0.4533 - learning_rate: 0.0066 - gradient_norm: 1.7285 - val_det_loss: 1.4886 - val_cls_loss: 1.4265 - val_box_loss: 0.0012 - val_reg_l2_loss: 0.0652 - val_loss: 1.5538\n",
            "Epoch 17/40\n",
            "75/75 [==============================] - 18s 238ms/step - det_loss: 0.3671 - cls_loss: 0.2581 - box_loss: 0.0022 - reg_l2_loss: 0.0652 - loss: 0.4324 - learning_rate: 0.0062 - gradient_norm: 1.7597 - val_det_loss: 1.4152 - val_cls_loss: 1.3557 - val_box_loss: 0.0012 - val_reg_l2_loss: 0.0652 - val_loss: 1.4804\n",
            "Epoch 18/40\n",
            "75/75 [==============================] - 18s 234ms/step - det_loss: 0.3583 - cls_loss: 0.2531 - box_loss: 0.0021 - reg_l2_loss: 0.0652 - loss: 0.4236 - learning_rate: 0.0058 - gradient_norm: 1.7556 - val_det_loss: 1.4573 - val_cls_loss: 1.3893 - val_box_loss: 0.0014 - val_reg_l2_loss: 0.0652 - val_loss: 1.5225\n",
            "Epoch 19/40\n",
            "75/75 [==============================] - 20s 262ms/step - det_loss: 0.3425 - cls_loss: 0.2409 - box_loss: 0.0020 - reg_l2_loss: 0.0652 - loss: 0.4077 - learning_rate: 0.0054 - gradient_norm: 1.5200 - val_det_loss: 1.5397 - val_cls_loss: 1.4742 - val_box_loss: 0.0013 - val_reg_l2_loss: 0.0652 - val_loss: 1.6049\n",
            "Epoch 20/40\n",
            "75/75 [==============================] - 20s 265ms/step - det_loss: 0.3566 - cls_loss: 0.2520 - box_loss: 0.0021 - reg_l2_loss: 0.0652 - loss: 0.4218 - learning_rate: 0.0050 - gradient_norm: 1.7520 - val_det_loss: 1.5541 - val_cls_loss: 1.4847 - val_box_loss: 0.0014 - val_reg_l2_loss: 0.0652 - val_loss: 1.6193\n",
            "Epoch 21/40\n",
            "75/75 [==============================] - 17s 233ms/step - det_loss: 0.3352 - cls_loss: 0.2386 - box_loss: 0.0019 - reg_l2_loss: 0.0652 - loss: 0.4004 - learning_rate: 0.0046 - gradient_norm: 1.5610 - val_det_loss: 1.5679 - val_cls_loss: 1.4941 - val_box_loss: 0.0015 - val_reg_l2_loss: 0.0652 - val_loss: 1.6331\n",
            "Epoch 22/40\n",
            "75/75 [==============================] - 18s 239ms/step - det_loss: 0.3363 - cls_loss: 0.2381 - box_loss: 0.0020 - reg_l2_loss: 0.0652 - loss: 0.4015 - learning_rate: 0.0042 - gradient_norm: 1.6771 - val_det_loss: 1.6137 - val_cls_loss: 1.5598 - val_box_loss: 0.0011 - val_reg_l2_loss: 0.0652 - val_loss: 1.6789\n",
            "Epoch 23/40\n",
            "75/75 [==============================] - 18s 244ms/step - det_loss: 0.3334 - cls_loss: 0.2383 - box_loss: 0.0019 - reg_l2_loss: 0.0652 - loss: 0.3986 - learning_rate: 0.0038 - gradient_norm: 1.5835 - val_det_loss: 1.5472 - val_cls_loss: 1.4921 - val_box_loss: 0.0011 - val_reg_l2_loss: 0.0652 - val_loss: 1.6124\n",
            "Epoch 24/40\n",
            "75/75 [==============================] - 18s 236ms/step - det_loss: 0.3166 - cls_loss: 0.2249 - box_loss: 0.0018 - reg_l2_loss: 0.0652 - loss: 0.3818 - learning_rate: 0.0034 - gradient_norm: 1.5194 - val_det_loss: 1.5831 - val_cls_loss: 1.5221 - val_box_loss: 0.0012 - val_reg_l2_loss: 0.0652 - val_loss: 1.6483\n",
            "Epoch 25/40\n",
            "75/75 [==============================] - 20s 267ms/step - det_loss: 0.3202 - cls_loss: 0.2275 - box_loss: 0.0019 - reg_l2_loss: 0.0652 - loss: 0.3854 - learning_rate: 0.0030 - gradient_norm: 1.6647 - val_det_loss: 1.6214 - val_cls_loss: 1.5581 - val_box_loss: 0.0013 - val_reg_l2_loss: 0.0652 - val_loss: 1.6865\n",
            "Epoch 26/40\n",
            "75/75 [==============================] - 18s 240ms/step - det_loss: 0.3040 - cls_loss: 0.2189 - box_loss: 0.0017 - reg_l2_loss: 0.0652 - loss: 0.3692 - learning_rate: 0.0027 - gradient_norm: 1.4861 - val_det_loss: 1.6404 - val_cls_loss: 1.5831 - val_box_loss: 0.0011 - val_reg_l2_loss: 0.0652 - val_loss: 1.7055\n",
            "Epoch 27/40\n",
            "75/75 [==============================] - 18s 243ms/step - det_loss: 0.3018 - cls_loss: 0.2138 - box_loss: 0.0018 - reg_l2_loss: 0.0652 - loss: 0.3670 - learning_rate: 0.0023 - gradient_norm: 1.4401 - val_det_loss: 1.6252 - val_cls_loss: 1.5691 - val_box_loss: 0.0011 - val_reg_l2_loss: 0.0652 - val_loss: 1.6903\n",
            "Epoch 28/40\n",
            "75/75 [==============================] - 18s 240ms/step - det_loss: 0.3092 - cls_loss: 0.2207 - box_loss: 0.0018 - reg_l2_loss: 0.0652 - loss: 0.3744 - learning_rate: 0.0020 - gradient_norm: 1.4906 - val_det_loss: 1.6301 - val_cls_loss: 1.5736 - val_box_loss: 0.0011 - val_reg_l2_loss: 0.0652 - val_loss: 1.6953\n",
            "Epoch 29/40\n",
            "75/75 [==============================] - 18s 237ms/step - det_loss: 0.2992 - cls_loss: 0.2135 - box_loss: 0.0017 - reg_l2_loss: 0.0652 - loss: 0.3644 - learning_rate: 0.0017 - gradient_norm: 1.4512 - val_det_loss: 1.6542 - val_cls_loss: 1.5961 - val_box_loss: 0.0012 - val_reg_l2_loss: 0.0652 - val_loss: 1.7193\n",
            "Epoch 30/40\n",
            "75/75 [==============================] - 20s 273ms/step - det_loss: 0.2983 - cls_loss: 0.2139 - box_loss: 0.0017 - reg_l2_loss: 0.0652 - loss: 0.3634 - learning_rate: 0.0014 - gradient_norm: 1.4223 - val_det_loss: 1.6593 - val_cls_loss: 1.6055 - val_box_loss: 0.0011 - val_reg_l2_loss: 0.0651 - val_loss: 1.7244\n",
            "Epoch 31/40\n",
            "75/75 [==============================] - 18s 244ms/step - det_loss: 0.2970 - cls_loss: 0.2127 - box_loss: 0.0017 - reg_l2_loss: 0.0651 - loss: 0.3621 - learning_rate: 0.0011 - gradient_norm: 1.3551 - val_det_loss: 1.6269 - val_cls_loss: 1.5725 - val_box_loss: 0.0011 - val_reg_l2_loss: 0.0651 - val_loss: 1.6921\n",
            "Epoch 32/40\n",
            "75/75 [==============================] - 18s 239ms/step - det_loss: 0.3003 - cls_loss: 0.2153 - box_loss: 0.0017 - reg_l2_loss: 0.0651 - loss: 0.3654 - learning_rate: 8.8626e-04 - gradient_norm: 1.5214 - val_det_loss: 1.6325 - val_cls_loss: 1.5783 - val_box_loss: 0.0011 - val_reg_l2_loss: 0.0651 - val_loss: 1.6976\n",
            "Epoch 33/40\n",
            "75/75 [==============================] - 17s 233ms/step - det_loss: 0.3023 - cls_loss: 0.2166 - box_loss: 0.0017 - reg_l2_loss: 0.0651 - loss: 0.3675 - learning_rate: 6.7111e-04 - gradient_norm: 1.4826 - val_det_loss: 1.6410 - val_cls_loss: 1.5882 - val_box_loss: 0.0011 - val_reg_l2_loss: 0.0651 - val_loss: 1.7062\n",
            "Epoch 34/40\n",
            "75/75 [==============================] - 18s 235ms/step - det_loss: 0.2898 - cls_loss: 0.2100 - box_loss: 0.0016 - reg_l2_loss: 0.0651 - loss: 0.3549 - learning_rate: 4.8403e-04 - gradient_norm: 1.3603 - val_det_loss: 1.6512 - val_cls_loss: 1.5987 - val_box_loss: 0.0010 - val_reg_l2_loss: 0.0651 - val_loss: 1.7163\n",
            "Epoch 35/40\n",
            "75/75 [==============================] - 20s 271ms/step - det_loss: 0.2890 - cls_loss: 0.2098 - box_loss: 0.0016 - reg_l2_loss: 0.0651 - loss: 0.3541 - learning_rate: 3.2624e-04 - gradient_norm: 1.4671 - val_det_loss: 1.6433 - val_cls_loss: 1.5907 - val_box_loss: 0.0011 - val_reg_l2_loss: 0.0651 - val_loss: 1.7084\n",
            "Epoch 36/40\n",
            "75/75 [==============================] - 18s 236ms/step - det_loss: 0.2879 - cls_loss: 0.2093 - box_loss: 0.0016 - reg_l2_loss: 0.0651 - loss: 0.3531 - learning_rate: 1.9876e-04 - gradient_norm: 1.3335 - val_det_loss: 1.6450 - val_cls_loss: 1.5915 - val_box_loss: 0.0011 - val_reg_l2_loss: 0.0651 - val_loss: 1.7102\n",
            "Epoch 37/40\n",
            "75/75 [==============================] - 18s 237ms/step - det_loss: 0.3035 - cls_loss: 0.2187 - box_loss: 0.0017 - reg_l2_loss: 0.0651 - loss: 0.3686 - learning_rate: 1.0242e-04 - gradient_norm: 1.4339 - val_det_loss: 1.6493 - val_cls_loss: 1.5958 - val_box_loss: 0.0011 - val_reg_l2_loss: 0.0651 - val_loss: 1.7144\n",
            "Epoch 38/40\n",
            "75/75 [==============================] - 18s 239ms/step - det_loss: 0.3016 - cls_loss: 0.2185 - box_loss: 0.0017 - reg_l2_loss: 0.0651 - loss: 0.3667 - learning_rate: 3.7839e-05 - gradient_norm: 1.5105 - val_det_loss: 1.6508 - val_cls_loss: 1.5974 - val_box_loss: 0.0011 - val_reg_l2_loss: 0.0651 - val_loss: 1.7159\n",
            "Epoch 39/40\n",
            "75/75 [==============================] - 18s 244ms/step - det_loss: 0.3008 - cls_loss: 0.2169 - box_loss: 0.0017 - reg_l2_loss: 0.0651 - loss: 0.3660 - learning_rate: 5.4417e-06 - gradient_norm: 1.4071 - val_det_loss: 1.6518 - val_cls_loss: 1.5982 - val_box_loss: 0.0011 - val_reg_l2_loss: 0.0651 - val_loss: 1.7169\n",
            "Epoch 40/40\n",
            "75/75 [==============================] - 20s 267ms/step - det_loss: 0.2915 - cls_loss: 0.2106 - box_loss: 0.0016 - reg_l2_loss: 0.0651 - loss: 0.3566 - learning_rate: 5.4360e-06 - gradient_norm: 1.4279 - val_det_loss: 1.6519 - val_cls_loss: 1.5982 - val_box_loss: 0.0011 - val_reg_l2_loss: 0.0651 - val_loss: 1.7171\n"
          ]
        }
      ],
      "source": [
        "model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data,epochs = 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BzCHLWJ6h7q"
      },
      "source": [
        "**Step 4. Evaluate the model with the test data.**\n",
        "\n",
        "After training the object detection model using the images in the training dataset, use the remaining 25 images in the test dataset to evaluate how the model performs against new data it has never seen before.\n",
        "\n",
        "As the default batch size is 64, it will take 1 step to go through the 25 images in the test dataset.\n",
        "\n",
        "The evaluation metrics are same as [COCO](https://cocodataset.org/#detection-eval)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xmnl6Yy7ARn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dcff2d4-481d-473a-826f-225332de5f98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 9s 516ms/step\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AP': 0.059194047,\n",
              " 'AP50': 0.08142161,\n",
              " 'AP75': 0.07920792,\n",
              " 'AP_/bishop': 0.0,\n",
              " 'AP_/black-bishop': 0.7036907,\n",
              " 'AP_/black-king': 0.0,\n",
              " 'AP_/black-knight': 0.0,\n",
              " 'AP_/black-pawn': 0.0,\n",
              " 'AP_/black-queen': 0.006621662,\n",
              " 'AP_/black-rook': 0.0,\n",
              " 'AP_/white-bishop': 0.0,\n",
              " 'AP_/white-king': 1.6164882e-05,\n",
              " 'AP_/white-knight': 0.0,\n",
              " 'AP_/white-pawn': 0.0,\n",
              " 'AP_/white-queen': 0.0,\n",
              " 'AP_/white-rook': 0.0,\n",
              " 'APl': -1.0,\n",
              " 'APm': 0.059275143,\n",
              " 'APs': 0.0,\n",
              " 'ARl': -1.0,\n",
              " 'ARm': 0.07186869,\n",
              " 'ARmax1': 0.038920455,\n",
              " 'ARmax10': 0.07186869,\n",
              " 'ARmax100': 0.07186869,\n",
              " 'ARs': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "model.evaluate(validation_data,batch_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgCDMe0e6jlT"
      },
      "source": [
        "**Step 5.  Export as a TensorFlow Lite model.**\n",
        "\n",
        "Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is full integer quantization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "id": "PgdZlgRM5DgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f7f977e-db27-467f-ff8b-d160a7ccdcf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer (KerasLayer)    multiple                  3234464   \n",
            "                                                                 \n",
            " class_net/class-predict (Se  multiple                 8181      \n",
            " parableConv2D)                                                  \n",
            "                                                                 \n",
            " box_net/box-predict (Separa  multiple                 2916      \n",
            " bleConv2D)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,245,561\n",
            "Trainable params: 3,198,425\n",
            "Non-trainable params: 47,136\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm_UULdW7A9T"
      },
      "outputs": [],
      "source": [
        "TFLITE_FILENAME = 'efficientdet-lite-duck.tflite'\n",
        "LABELS_FILENAME = 'duck-labels.txt'\n",
        "model.export(export_dir='.', tflite_filename=TFLITE_FILENAME, label_filename=LABELS_FILENAME,\n",
        "             export_format=[ExportFormat.TFLITE, ExportFormat.LABEL])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQpahAIBqBPp"
      },
      "source": [
        "**Step 6.  Evaluate the TensorFlow Lite model.**\n",
        "\n",
        "Several factors can affect the model accuracy when exporting to TFLite:\n",
        "* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop.\n",
        "* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\n",
        "Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n",
        "\n",
        "Therefore you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS3Ell_lqH4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "outputId": "118fc0eb-9397-4430-9cfa-b92635ad23d9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-17c2959e565b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_tflite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFLITE_FILENAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
          ]
        }
      ],
      "source": [
        "model.evaluate_tflite(TFLITE_FILENAME, test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVxaf3x_7OfB"
      },
      "source": [
        "You can download the TensorFlow Lite model file using the left sidebar of Colab. Right-click on the `model.tflite` file and choose `Download` to download it to your local computer.\n",
        "\n",
        "This model can be integrated into an Android or an iOS app using the [ObjectDetector API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/object_detector) of the [TensorFlow Lite Task Library](https://www.tensorflow.org/lite/inference_with_metadata/task_library/overview).\n",
        "\n",
        "See the [TFLite Object Detection sample app](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/lib_task_api/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java#L91) for more details on how the model is used in an working app.\n",
        "\n",
        "*Note: Android Studio Model Binding does not support object detection yet so please use the TensorFlow Lite Task Library.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me6_RwPZqNhX"
      },
      "source": [
        "## (Optional) Test the TFLite model on your image\n",
        "\n",
        "You can test the trained TFLite model using images from the internet.\n",
        "* Replace the `INPUT_IMAGE_URL` below with your desired input image.\n",
        "* Adjust the `DETECTION_THRESHOLD` to change the sensitivity of the model. A lower threshold means the model will pickup more objects but there will also be more false detection. Meanwhile, a higher threshold means the model will only pickup objects that it has confidently detected.\n",
        "\n",
        "Although it requires some of boilerplate code to run the model in Python at this moment, integrating the model into a mobile app only requires a few lines of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqS0rFCrqM1o"
      },
      "outputs": [],
      "source": [
        "#@title Load the trained TFLite model and define some visualization functions\n",
        "\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "model_path = TFLITE_FILENAME\n",
        "\n",
        "# Load the labels into a list\n",
        "classes = ['???'] * model.model_spec.config.num_classes\n",
        "label_map = model.model_spec.config.label_map\n",
        "for label_id, label_name in label_map.as_dict().items():\n",
        "  classes[label_id-1] = label_name\n",
        "\n",
        "# Define a list of colors for visualization\n",
        "COLORS = np.random.randint(0, 255, size=(len(classes), 3), dtype=np.uint8)\n",
        "\n",
        "def preprocess_image(image_path, input_size):\n",
        "  \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\n",
        "  img = tf.io.read_file(image_path)\n",
        "  img = tf.io.decode_image(img, channels=3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.uint8)\n",
        "  original_image = img\n",
        "  resized_img = tf.image.resize(img, input_size)\n",
        "  resized_img = resized_img[tf.newaxis, :]\n",
        "  resized_img = tf.cast(resized_img, dtype=tf.uint8)\n",
        "  return resized_img, original_image\n",
        "\n",
        "\n",
        "def detect_objects(interpreter, image, threshold):\n",
        "  \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
        "\n",
        "  signature_fn = interpreter.get_signature_runner()\n",
        "\n",
        "  # Feed the input image to the model\n",
        "  output = signature_fn(images=image)\n",
        "\n",
        "  # Get all outputs from the model\n",
        "  count = int(np.squeeze(output['output_0']))\n",
        "  scores = np.squeeze(output['output_1'])\n",
        "  classes = np.squeeze(output['output_2'])\n",
        "  boxes = np.squeeze(output['output_3'])\n",
        "\n",
        "  results = []\n",
        "  for i in range(count):\n",
        "    if scores[i] >= threshold:\n",
        "      result = {\n",
        "        'bounding_box': boxes[i],\n",
        "        'class_id': classes[i],\n",
        "        'score': scores[i]\n",
        "      }\n",
        "      results.append(result)\n",
        "  return results\n",
        "\n",
        "\n",
        "def run_odt_and_draw_results(image_path, interpreter, threshold=0.5):\n",
        "  \"\"\"Run object detection on the input image and draw the detection results\"\"\"\n",
        "  # Load the input shape required by the model\n",
        "  _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n",
        "\n",
        "  # Load the input image and preprocess it\n",
        "  preprocessed_image, original_image = preprocess_image(\n",
        "      image_path,\n",
        "      (input_height, input_width)\n",
        "    )\n",
        "\n",
        "  # Run object detection on the input image\n",
        "  results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
        "\n",
        "  # Plot the detection results on the input image\n",
        "  original_image_np = original_image.numpy().astype(np.uint8)\n",
        "  for obj in results:\n",
        "    # Convert the object bounding box from relative coordinates to absolute\n",
        "    # coordinates based on the original image resolution\n",
        "    ymin, xmin, ymax, xmax = obj['bounding_box']\n",
        "    xmin = int(xmin * original_image_np.shape[1])\n",
        "    xmax = int(xmax * original_image_np.shape[1])\n",
        "    ymin = int(ymin * original_image_np.shape[0])\n",
        "    ymax = int(ymax * original_image_np.shape[0])\n",
        "\n",
        "    # Find the class index of the current object\n",
        "    class_id = int(obj['class_id'])\n",
        "\n",
        "    # Draw the bounding box and label on the image\n",
        "    color = [int(c) for c in COLORS[class_id]]\n",
        "    cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)\n",
        "    # Make adjustments to make the label visible for all objects\n",
        "    y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
        "    label = \"{}: {:.0f}%\".format(classes[class_id], obj['score'] * 100)\n",
        "    cv2.putText(original_image_np, label, (xmin, y),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "  # Return the final image\n",
        "  original_uint8 = original_image_np.astype(np.uint8)\n",
        "  return original_uint8"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "TjNp6nVkwSEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkXtipXKqXp4"
      },
      "outputs": [],
      "source": [
        "#@title Run object detection and show the detection results\n",
        "\n",
        "INPUT_IMAGE_URL = \"bb0de9761d16eee258ae09d8de32002c_jpg.rf.f97067be0b27db05f0ac03d9d8cb1185.jpg\" #@param {type:\"string\"}\n",
        "DETECTION_THRESHOLD = 0.3 #@param {type:\"number\"}\n",
        "\n",
        "TEMP_FILE = '/tmp/image.png'\n",
        "\n",
        "!wget -q -O $TEMP_FILE $INPUT_IMAGE_URL\n",
        "im = Image.open(INPUT_IMAGE_URL)\n",
        "im.thumbnail((512, 512), Image.ANTIALIAS)\n",
        "im.save(TEMP_FILE, 'PNG')\n",
        "\n",
        "# Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Run inference and draw detection result on the local copy of the original file\n",
        "detection_result_image = run_odt_and_draw_results(\n",
        "    TEMP_FILE,\n",
        "    interpreter,\n",
        "    threshold=DETECTION_THRESHOLD\n",
        ")\n",
        "\n",
        "# Show the detection result\n",
        "Image.fromarray(detection_result_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxgWQyYOqZha"
      },
      "source": [
        "## (Optional) Compile For the Edge TPU\n",
        "\n",
        "Now that you have a quantized EfficientDet Lite model, it is possible to compile and deploy to a [Coral EdgeTPU](https://coral.ai/).\n",
        "\n",
        "**Step 1. Install the EdgeTPU Compiler**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy3QIn_YqaRP"
      },
      "outputs": [],
      "source": [
        "! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "\n",
        "! echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "\n",
        "! sudo apt-get update\n",
        "\n",
        "! sudo apt-get install edgetpu-compiler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRWewhqFqeL_"
      },
      "source": [
        "**Step 2. Select number of Edge TPUs, Compile**\n",
        "\n",
        "The EdgeTPU has 8MB of SRAM for caching model paramaters ([more info](https://coral.ai/docs/edgetpu/compiler/#parameter-data-caching)). This means that for models that are larger than 8MB, inference time will be increased in order to transfer over model paramaters. One way to avoid this is [Model Pipelining](https://coral.ai/docs/edgetpu/pipeline/) - splitting the model into segments that can have a dedicated EdgeTPU. This can significantly improve latency.\n",
        "\n",
        "The below table can be used as a reference for the number of Edge TPUs to use - the larger models will not compile for a single TPU as the intermediate tensors can't fit in on-chip memory.\n",
        "\n",
        "| Model architecture | Minimum TPUs | Recommended TPUs\n",
        "|--------------------|-------|-------|\n",
        "| EfficientDet-Lite0 | 1     | 1     |\n",
        "| EfficientDet-Lite1 | 1     | 1     |\n",
        "| EfficientDet-Lite2 | 1     | 2     |\n",
        "| EfficientDet-Lite3 | 2     | 2     |\n",
        "| EfficientDet-Lite4 | 2     | 3     |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LZdonJGCqieU"
      },
      "outputs": [],
      "source": [
        "NUMBER_OF_TPUS =  1#@param {type:\"number\"}\n",
        "\n",
        "!edgetpu_compiler model.tflite --num_segments=$NUMBER_OF_TPUS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g6_KQXnqlTC"
      },
      "source": [
        "**Step 3. Download, Run Model**\n",
        "\n",
        "With the model(s) compiled, they can now be run on EdgeTPU(s) for object detection. First, download the compiled TensorFlow Lite model file using the left sidebar of Colab. Right-click on the `model_edgetpu.tflite` file and choose `Download` to download it to your local computer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkQFz_qzqrrA"
      },
      "source": [
        "Now you can run the model in your preferred manner. Examples of detection include:\n",
        "* [pycoral detection](https://github.com/google-coral/pycoral/blob/master/examples/detect_image.py)\n",
        "* [Basic TFLite detection](https://github.com/google-coral/tflite/tree/master/python/examples/detection)\n",
        "* [Example Video Detection](https://github.com/google-coral/examples-camera)\n",
        "* [libcoral C++ API](https://github.com/google-coral/libcoral)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoWiA_zX8rxE"
      },
      "source": [
        "## Advanced Usage\n",
        "\n",
        "This section covers advanced usage topics like adjusting the model and the training hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p79NHCx0xFqb"
      },
      "source": [
        "### Load the dataset\n",
        "\n",
        "#### Load your own data\n",
        "\n",
        "You can upload your own dataset to work through this tutorial. Upload your dataset by using the left sidebar in Colab.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/download.tensorflow.org/models/tflite/screenshots/model_maker_object_detection.png\" alt=\"Upload File\" width=\"1000\" hspace=\"0\">\n",
        "\n",
        "If you prefer not to upload your dataset to the cloud, you can also locally run the library by following the [guide](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker).\n",
        "\n",
        "#### Load your data with a different data format\n",
        "\n",
        "The Model Maker library also supports the `object_detector.DataLoader.from_pascal_voc` method to load data with [PASCAL VOC](https://towardsdatascience.com/coco-data-format-for-object-detection-a4c5eaf518c5#:~:text=Pascal%20VOC%20is%20an%20XML,for%20training%2C%20testing%20and%20validation) format. [makesense.ai](https://www.makesense.ai/) and [LabelImg](https://github.com/tzutalin/labelImg) are the tools that can annotate the image and save annotations as XML files in PASCAL VOC data format:\n",
        "```python\n",
        "object_detector.DataLoader.from_pascal_voc(image_dir, annotations_dir, label_map={1: \"person\", 2: \"notperson\"})\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8VxPiOLy4Gv"
      },
      "source": [
        "### Customize the EfficientDet model hyperparameters\n",
        "\n",
        "The model and training pipline parameters you can adjust are:\n",
        "\n",
        "* `model_dir`: The location to save the model checkpoint files. If not set, a temporary directory will be used.\n",
        "* `steps_per_execution`: Number of steps per training execution.\n",
        "* `moving_average_decay`: Float. The decay to use for maintaining moving averages of the trained parameters.\n",
        "* `var_freeze_expr`: The regular expression to map the prefix name of variables to be frozen which means remaining the same during training. More specific, use `re.match(var_freeze_expr, variable_name)` in the codebase to map the variables to be frozen.\n",
        "* `tflite_max_detections`: integer, 25 by default. The max number of output detections in the TFLite model.\n",
        "* `strategy`:  A string specifying which distribution strategy to use. Accepted values are 'tpu', 'gpus', None. tpu' means to use TPUStrategy. 'gpus' mean to use MirroredStrategy for multi-gpus. If None, use TF default with OneDeviceStrategy.\n",
        "* `tpu`:  The Cloud TPU to use for training. This should be either the name used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url.\n",
        "* `use_xla`: Use XLA even if strategy is not tpu. If strategy is tpu, always use XLA, and this flag has no effect.\n",
        "* `profile`: Enable profile mode.\n",
        "* `debug`: Enable debug mode.\n",
        "\n",
        "Other parameters that can be adjusted is shown in [hparams_config.py](https://github.com/google/automl/blob/df451765d467c5ed78bbdfd632810bc1014b123e/efficientdet/hparams_config.py#L170).\n",
        "\n",
        "\n",
        "For instance, you can set the `var_freeze_expr='efficientnet'` which freezes the variables with name prefix `efficientnet` (default is `'(efficientnet|fpn_cells|resample_p6)'`). This allows the model to freeze untrainable variables and keep their value the same through training.\n",
        "\n",
        "```python\n",
        "spec = model_spec.get('efficientdet_lite0')\n",
        "spec.config.var_freeze_expr = 'efficientnet'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J2qre1fwXsi"
      },
      "source": [
        "### Change the Model Architecture\n",
        "\n",
        "You can change the model architecture by changing the `model_spec`. For instance, change the `model_spec` to the EfficientDet-Lite4 model.\n",
        "\n",
        "```python\n",
        "spec = model_spec.get('efficientdet_lite4')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvQuy7RSDir3"
      },
      "source": [
        "### Tune the training hyperparameters\n",
        "\n",
        "The `create` function is the driver function that the Model Maker library uses to create models. The `model_spec` parameter defines the model specification. The `object_detector.EfficientDetSpec` class is currently supported. The `create` function comprises of the following steps:\n",
        "\n",
        "1. Creates the model for the object detection according to `model_spec`.\n",
        "2. Trains the model.  The default epochs and the default batch size are set by the `epochs` and `batch_size` variables in the `model_spec` object.\n",
        "You can also tune the training hyperparameters like `epochs` and `batch_size` that affect the model accuracy. For instance,\n",
        "\n",
        "*   `epochs`: Integer, 50 by default. More epochs could achieve better accuracy, but may lead to overfitting.\n",
        "*   `batch_size`: Integer, 64 by default. The number of samples to use in one training step.\n",
        "*   `train_whole_model`: Boolean, False by default. If true, train the whole model. Otherwise, only train the layers that do not match `var_freeze_expr`.\n",
        "\n",
        "For example, you can train with less epochs and only the head layer. You can increase the number of epochs for better results.\n",
        "\n",
        "```python\n",
        "model = object_detector.create(train_data, model_spec=spec, epochs=10, validation_data=validation_data)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vPyZInPxJBT"
      },
      "source": [
        "### Export to different formats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xqNIcBM-4YR"
      },
      "source": [
        "The export formats can be one or a list of the following:\n",
        "\n",
        "*   `ExportFormat.TFLITE`\n",
        "*   `ExportFormat.LABEL`\n",
        "*   `ExportFormat.SAVED_MODEL`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enhsZhW3ApcX"
      },
      "source": [
        "By default, it exports only the TensorFlow Lite model file containing the model [metadata](https://www.tensorflow.org/lite/models/convert/metadata) so that you can later use in an on-device ML application. The label file is embedded in metadata.\n",
        "\n",
        "In many on-device ML application, the model size is an important factor. Therefore, it is recommended that you quantize the model to make it smaller and potentially run faster. As for EfficientDet-Lite models, full integer quantization  is used to quantize the model by default. Please refer to [Post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) for more detail.\n",
        "\n",
        "```python\n",
        "model.export(export_dir='.')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLGZs6InAnP5"
      },
      "source": [
        "You can also choose to export other files related to the model for better examination. For instance, exporting both the saved model and the label file as follows:\n",
        "```python\n",
        "model.export(export_dir='.', export_format=[ExportFormat.SAVED_MODEL, ExportFormat.LABEL])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5q_McchQ2C4"
      },
      "source": [
        "### Customize Post-training quantization on the TensorFlow Lite model\n",
        "\n",
        "[Post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) is a conversion technique that can reduce model size and inference latency, while also improving CPU and hardware accelerator inference speed, with a little degradation in model accuracy. Thus, it's widely used to optimize the model.\n",
        "\n",
        "Model Maker library applies a default post-training quantization techique when exporting the model. If you want to customize post-training quantization, Model Maker supports multiple post-training quantization options using [QuantizationConfig](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/config/QuantizationConfig) as well. Let's take float16 quantization as an instance. First, define the quantization config.\n",
        "\n",
        "```python\n",
        "config = QuantizationConfig.for_float16()\n",
        "```\n",
        "\n",
        "\n",
        "Then we export the TensorFlow Lite model with such configuration.\n",
        "\n",
        "```python\n",
        "model.export(export_dir='.', tflite_filename='model_fp16.tflite', quantization_config=config)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS4u77W5gnzQ"
      },
      "source": [
        "# Read more\n",
        "\n",
        "You can read our [object detection](https://www.tensorflow.org/lite/examples/object_detection/overview) example to learn technical details. For more information, please refer to:\n",
        "\n",
        "*   TensorFlow Lite Model Maker [guide](https://www.tensorflow.org/lite/models/modify/model_maker) and [API reference](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker).\n",
        "*   Task Library: [ObjectDetector](https://www.tensorflow.org/lite/inference_with_metadata/task_library/object_detector) for deployment.\n",
        "*   The end-to-end reference apps: [Android](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android), [iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/ios), and [Raspberry PI](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/raspberry_pi).\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "A nice game of chess",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}